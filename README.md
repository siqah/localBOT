# LocalBOT â€” Private AI Desktop Assistant

> **Your private AI assistant that runs 100% offline â€” secure, fast, and works with all your local files.**

## ğŸ” The Problem

Most AI assistants (ChatGPT, Copilot, Claude) require sending your data to cloud servers. This creates serious concerns when working with:

- **Sensitive documents** â€” contracts, financial records, legal files, medical data
- **Proprietary knowledge** â€” internal docs, trade secrets, research notes
- **Regulated environments** â€” industries with strict data residency requirements
- **Personal privacy** â€” users who simply don't want their data leaving their machine

## ğŸ’¡ The Solution

LocalBOT is a standalone, cross-platform **Electron desktop app** that lets you chat with your documents using a fully local **RAG (Retrieval-Augmented Generation)** pipeline. No cloud, no API keys, no internet â€” just download and run.

Your documents are parsed, chunked, embedded, and indexed entirely on your machine. When you ask a question, the AI retrieves the most relevant passages from your knowledge base and generates an answer â€” all locally.

## âœ¨ Features

- ğŸ“„ **Document Management** â€” Upload and manage PDF, DOCX, TXT, Markdown, CSV, JSON, YAML, and HTML files
- ğŸ§  **Local AI Models** â€” Embeddings via Transformers.js + LLM inference via node-llama-cpp (no API keys needed)
- ğŸ” **Semantic Search** â€” Find relevant information across all your documents using vector similarity (not just keyword matching)
- ğŸ’¬ **Chat with Citations** â€” Ask questions and get AI-generated answers with source references pointing to exact document passages
- ğŸ”’ **100% Offline** â€” All processing happens on your device. Zero data ever leaves your machine
- ğŸ–¥ï¸ **Cross-Platform** â€” Runs on macOS, Windows, and Linux
- ğŸ¨ **Modern UI** â€” Clean, responsive React interface with dark/light mode

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Electron Shell                    â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Frontend    â”‚  IPC   â”‚     Main Process      â”‚  â”‚
â”‚  â”‚  React + TS   â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚                        â”‚  â”‚
â”‚  â”‚  Tailwind CSS â”‚        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚   RAG Pipeline    â”‚ â”‚  â”‚
â”‚                           â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”‚
â”‚                           â”‚  â”‚  â”‚  Parser      â”‚  â”‚ â”‚  â”‚
â”‚                           â”‚  â”‚  â”‚  Chunker     â”‚  â”‚ â”‚  â”‚
â”‚                           â”‚  â”‚  â”‚  Embedder    â”‚  â”‚ â”‚  â”‚
â”‚                           â”‚  â”‚  â”‚  LLM         â”‚  â”‚ â”‚  â”‚
â”‚                           â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â”‚
â”‚                           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚                           â”‚                        â”‚  â”‚
â”‚                           â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚                           â”‚  â”‚SQLiteâ”‚ â”‚ Vectra  â”‚  â”‚  â”‚
â”‚                           â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Shell** | Electron | Cross-platform desktop runtime |
| **Frontend** | React + TypeScript + Tailwind CSS | User interface |
| **Database** | SQLite (`better-sqlite3`) | Document metadata, chat sessions, audit logs |
| **Cache** | In-memory LRU Cache (`lru-cache`) | Response caching |
| **Vector DB** | Vectra (local JSON-based) | Embedding storage & similarity search |
| **Embeddings** | `@xenova/transformers` (`all-MiniLM-L6-v2`) | Document & query embedding |
| **LLM** | `node-llama-cpp` (GGUF models) | Local text generation |
| **IPC** | Electron `ipcMain` / `ipcRenderer` | Secure frontend â†” backend communication |

## ğŸš€ Getting Started

### Prerequisites

- **Node.js** 20+
- **npm** 9+
- A GGUF model file (e.g., [TinyLlama 1.1B Chat](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF))

### Install & Run

```bash
# 1. Clone the repository
git clone <repo-url>
cd localBOT

# 2. Install backend dependencies
npm install

# 3. Install frontend dependencies
cd frontend && npm install && cd ..

# 4. Download a GGUF model (example: TinyLlama)
#    Place the .gguf file in:
#    macOS:  ~/Library/Application Support/localbot/models/
#    Win:    %APPDATA%/localbot/models/
#    Linux:  ~/.config/localbot/models/

# 5. Start in development mode
npm run dev
```

The app will launch an Electron window with the React frontend. The Vite dev server runs on `http://localhost:5173`.

### Build for Distribution

```bash
# Build React bundle + package Electron app
npm run build
```

Distributable installers are generated in `dist/`:
- **macOS**: `.dmg`, `.zip`
- **Windows**: `.exe` (NSIS), `.zip`
- **Linux**: `.AppImage`, `.deb`

## ğŸ“ Project Structure

```
localBOT/
â”œâ”€â”€ main.js                  # Electron main process â€” app lifecycle & window
â”œâ”€â”€ preload.js               # Secure IPC bridge (context isolation)
â”œâ”€â”€ package.json             # Root config & electron-builder settings
â”œâ”€â”€ migrations/
â”‚   â””â”€â”€ 001_init.sql         # SQLite schema (documents, chunks, sessions, etc.)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ipcHandlers.js       # All IPC route handlers (documents, chat, search, system)
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â””â”€â”€ redis.js         # LRU in-memory cache (Redis API-compatible interface)
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ database.js      # SQLite connection & migration runner
â”‚   â”‚   â””â”€â”€ repository.js    # Data access layer (CRUD for documents, sessions, etc.)
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ parser.js        # File parser (PDF, DOCX, HTML, CSV, YAML, etc.)
â”‚   â”‚   â”œâ”€â”€ pipeline.js      # RAG orchestration (chunk â†’ embed â†’ index â†’ query)
â”‚   â”‚   â”œâ”€â”€ elasticsearch.js # Vectra vector store (similarity search)
â”‚   â”‚   â””â”€â”€ localai.js       # AI models (Transformers.js embeddings + Llama LLM)
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.js        # Winston logger
â””â”€â”€ frontend/                # React + Vite + Tailwind UI
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.tsx           # Main layout with sidebar navigation
    â”‚   â”œâ”€â”€ main.tsx          # React entry point
    â”‚   â”œâ”€â”€ index.css         # Tailwind + custom design system
    â”‚   â”œâ”€â”€ lib/
    â”‚   â”‚   â””â”€â”€ api.ts        # IPC client â€” typed API for all backend calls
    â”‚   â””â”€â”€ pages/
    â”‚       â”œâ”€â”€ ChatView.tsx       # Chat interface with source citations
    â”‚       â”œâ”€â”€ DocumentsView.tsx  # Document upload, list & management
    â”‚       â”œâ”€â”€ KnowledgeView.tsx  # Semantic search explorer
    â”‚       â””â”€â”€ SettingsView.tsx   # System health, stats & RAG config
    â””â”€â”€ vite.config.ts        # Vite build configuration
```

## ğŸ”„ How It Works

1. **Upload** â€” Drop a document (PDF, DOCX, etc.) into the app
2. **Parse** â€” The file is converted to plain text using format-specific parsers
3. **Chunk** â€” Text is split into overlapping chunks for better retrieval
4. **Embed** â€” Each chunk is embedded into a 384-dim vector using `all-MiniLM-L6-v2`
5. **Index** â€” Embeddings are stored in the local Vectra vector database
6. **Query** â€” When you ask a question, your query is embedded and the most similar chunks are retrieved
7. **Generate** â€” The retrieved context + your question are sent to the local LLM, which generates an answer with citations

## ğŸ›¡ï¸ Privacy & Security

- **No network calls** â€” The app makes zero HTTP requests. Everything runs locally
- **Context isolation** â€” The Electron frontend runs in a sandboxed renderer with `contextIsolation: true`
- **Whitelisted IPC** â€” Only pre-approved IPC channels can be invoked from the frontend
- **Local storage** â€” All data (SQLite DB, vector index, models) is stored in the OS user data directory

## ğŸ“œ License

MIT
